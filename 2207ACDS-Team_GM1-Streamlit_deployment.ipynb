{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Classification Predict Student Solution\n",
    "\n",
    "\n",
    "### Predict Overview: Twitter Sentiment Classification\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received. Your company has been awarded the contract to:\n",
    "\n",
    "- 1. Create a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "- 2. Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine what features were most important in the model’s prediction decision, and\n",
    "- 7. explain the inner working of the model to a non-technical audience.\n",
    "\n",
    "Formally the problem statement was given to the data science team, by your manager via email reads as follow:\n",
    "\n",
    "> The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43,943 tweets were collected. Each tweet is labelled as one of 4 classes, which are described below:\n",
    "\n",
    "> - 2 News: the tweet links to factual news about climate change\n",
    "\n",
    "> - 1 Pro: the tweet supports the belief of man-made climate change\n",
    "\n",
    "> - 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "\n",
    "> - -1 Anti: the tweet does not believe in man-made climate change Variable definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475dbe93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "sns.set()\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "import string\n",
    "import preprocessor as p\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Model importation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "#PARAMETER_CONSTANT = ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbbb6c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('resources/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78139e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @darreljorstad: Funny as hell! Canada deman...</td>\n",
       "      <td>897853122080407553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>All the biggest lies about climate change and ...</td>\n",
       "      <td>925046776553529344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>The Coming Revelation Of The $q$Global Warming...</td>\n",
       "      <td>696354236850786305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @DineshDSouza: Let's see if the world ends ...</td>\n",
       "      <td>846806509732483072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @SteveSGoddard: Obama has no control over t...</td>\n",
       "      <td>628085266293653504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30754</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @TIME: The Pentagon warned that climate cha...</td>\n",
       "      <td>958155326259367937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30755</th>\n",
       "      <td>2</td>\n",
       "      <td>Study finds that global warming exacerbates re...</td>\n",
       "      <td>956048238615900163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30756</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @MikeySlezak: The global green movement pre...</td>\n",
       "      <td>800258621485391872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30757</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @ProfEdwardsNZ: NYC Mayor says NY will go f...</td>\n",
       "      <td>871365767895404545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30758</th>\n",
       "      <td>2</td>\n",
       "      <td>RT Sanders: Climate Change Causes Terrorism  h...</td>\n",
       "      <td>666236638155141121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30759 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  \\\n",
       "0             -1  RT @darreljorstad: Funny as hell! Canada deman...   \n",
       "1             -1  All the biggest lies about climate change and ...   \n",
       "2             -1  The Coming Revelation Of The $q$Global Warming...   \n",
       "3             -1  RT @DineshDSouza: Let's see if the world ends ...   \n",
       "4             -1  RT @SteveSGoddard: Obama has no control over t...   \n",
       "...          ...                                                ...   \n",
       "30754          2  RT @TIME: The Pentagon warned that climate cha...   \n",
       "30755          2  Study finds that global warming exacerbates re...   \n",
       "30756          2  RT @MikeySlezak: The global green movement pre...   \n",
       "30757          2  RT @ProfEdwardsNZ: NYC Mayor says NY will go f...   \n",
       "30758          2  RT Sanders: Climate Change Causes Terrorism  h...   \n",
       "\n",
       "                  tweetid  \n",
       "0      897853122080407553  \n",
       "1      925046776553529344  \n",
       "2      696354236850786305  \n",
       "3      846806509732483072  \n",
       "4      628085266293653504  \n",
       "...                   ...  \n",
       "30754  958155326259367937  \n",
       "30755  956048238615900163  \n",
       "30756  800258621485391872  \n",
       "30757  871365767895404545  \n",
       "30758  666236638155141121  \n",
       "\n",
       "[30759 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display all columns of the dataframe\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Feature Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "In this section, observations from the EDA will be addressed, futhermore, new features that are necessary for the model will be created.\n",
    "\n",
    "To deal with the data quality issues, tweet-preprocessor library will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de93c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.set_options(p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.ESCAPE_CHAR,p.OPT.SMILEY, p.OPT.HASHTAG,p.OPT.NUMBER,p.OPT.URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd2a8c",
   "metadata": {},
   "source": [
    "#### Create a Function for Data Cleaning\n",
    "\n",
    "The function created will automate the following process:\n",
    "- Replace all ULRs with 'url-web'\n",
    "- Clean each row by removing mention, hashtags, emoji and other irrelevant text components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf2f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "    # regex to search for all URL in the DataFrame\n",
    "    pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "    subs_url = r'url-web'\n",
    "    \n",
    "    # Replace all URL with the subs_url\n",
    "    #df['regex_cleaned_message'] = df['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "    df['message'] = df['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "    \n",
    "    # Use tweet-preprocessor to lean each row of the dataframe based on the settings in the p.set_options\n",
    "    df['message'] = df['message'].apply(p.clean)\n",
    "    \n",
    "    df = df.drop('tweetid', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf39c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = clean_data(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc285bdc",
   "metadata": {},
   "source": [
    "#### Create a Pipeline to Preprocess for Feature Engineering\n",
    "\n",
    "The pipiline created will automate the following phases:\n",
    "- Remove punctuations\n",
    "- Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a9114",
   "metadata": {},
   "source": [
    "Load SpaCY NLP medium model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1d03c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec9b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of stopwords from spacy framework\n",
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48789e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(message):\n",
    "    message = nlp(message)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    message = [token for token in message if not token.is_punct]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    message = [token for token in message if not token.is_stop]\n",
    "    \n",
    "    #lemmatize each word and convert them to lower case\n",
    "    message = \" \".join([token.lemma_.lower() for token in message])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return message\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67678a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_train_clean['message']\n",
    "target = df_train_clean['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84eea17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorzer Object\n",
    "vect = TfidfVectorizer(stop_words='english', max_features=100, ngram_range=(1,2))\n",
    "trained_vect = vect.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09955dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP PC\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trained_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f5c5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trained_vect.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8804160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30759, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = X_train.toarray()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf4a010d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30759, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f6772",
   "metadata": {},
   "source": [
    "#### Split the data into train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14fcd8e",
   "metadata": {},
   "source": [
    "Given that we rarely have a population to work with, and also, it is computationally expensive to do so, we ought to ensure that we have a means of testing our models on UNSEEN data. i.e. data that wasn't part of the training components so as to be ables to measure the predicitive accuracy of our model.\n",
    "\n",
    "The essence of this is to ensure that our model do not fit so well with the training data and nece fail in generalising the population, giving rise to what is known as OVERFITTING(a situation where we have less error during training but very high error in the testing phase).\n",
    "\n",
    "To achieve this, we tend to split our available data set into training and testing data, where the training data are used to train the model while the testing data is used to test the accuracy of our model.\n",
    "Where the data set is large enough, it can be divided into three sets adding the Validation set to the training and testing set.\n",
    ">\n",
    "To split our data set, sklearn has a very easy function to help us achieve that, the train_test_split function. The function takes in an array or df and splits them at random(although the random state can be maintained) and returns 2n number of outputs. Where n = number of arrays/dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fe7f71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data with a test-size of 0.2 and random state of 42 for reproduceability\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3519fe5",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a7da1",
   "metadata": {},
   "source": [
    "#### Create a Word2Vector object\n",
    "\n",
    "The vectorizer is used to transform a given text into a vector on the basis of the frequency of each word that occurs in the entire text.\n",
    "\n",
    "For this project, CountVectorizer will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fecc6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorzer Object\n",
    "vect = CountVectorizer(tokenizer=pipeline, min_df=10, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd221d02",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic regression is a classification algorith, that predicts the probability of an event occurring using a logistic function. This implies that the outcome is not numerical but rather, categorical.\n",
    "\n",
    "To explain further with an example, if Linear Regression predicts how much a customer is willing to pay if they buy a company's product, then Logistic regression predicts whether a customer will buy a company's product or not.\n",
    "As explained above, Logistic Regression can be used to make very fundamental forecast.\n",
    "\n",
    "$$P(X) = \\displaystyle \\frac{e^{\\beta_0 + \\beta_1 X}}{1+e^{\\beta_0 + \\beta_1 X}}$$\n",
    "\n",
    "where $P(X)$ is the probability of X belonging to class 1, and $\\beta_0$ and $\\beta_1$ are the intercept and regression coefficient respectively, just like in a linear regression model.\n",
    "\n",
    "Logistic regression can transform into its logit form, where the log of the odds is equal to a linear model.\n",
    "\n",
    "\\begin{align}\n",
    "1 - P(X) &= \\displaystyle \\frac{1}{1+e^{\\beta_0 + \\beta_1 X}} \\\\\n",
    "\\therefore \\log \\left( \\frac{P(X)}{1-P(X)} \\right) &= {\\beta_0 + \\beta_1 X}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "Where \\left( \\frac{P(X)}{1-P(X)} \\right) = Odds\n",
    "\\end{align}\n",
    "\n",
    "Hence, the logit function can be re-written as\n",
    "\n",
    "\\begin{align}\n",
    "\\therefore \\log \\left( Odds \\right) &= {\\beta_0 + \\beta_1 X}\n",
    "\\end{align}\n",
    "\n",
    "Since Logistic Regression predicts the probability of an event occuring, its prediction is between 0 and 1. Usually, a threshold is picked above which classification is assigned 1 and below which it is assigned 0. 0.5 is normally chosen.\n",
    "\n",
    "Although usually used for bianry classification, Logistic regression can be used for multi class classification. This can be achieved in sklearn by setting the Logistic Regression parameter \"mult_class\" to \"ovr\" which means One-vs-Rest.\n",
    "The intuition behind the ovr is the same with binary classification, only that in ovr the possible outcomes are:\n",
    "- The probability of the event occurring (p), and\n",
    "- The probability of the event not occuring (1 - p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b7998c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a LogisticRegression object\n",
    "logreg = LogisticRegression(penalty='l2', C= 0.1, solver='liblinear', multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7904141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "logistic_reg = logreg.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcbcf250",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"resources/logistic_reg.pkl\"\n",
    "\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(logistic_reg,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff9cd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"resources/countvect.pkl\"\n",
    "\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(trained_vect,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c4a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a750866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best model and motivate why it is the best choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad0c0d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff741c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss chosen methods logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae9db3",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3680ae2",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8e91f",
   "metadata": {},
   "source": [
    "[Tweet-Preprocessor](https://pypi.org/project/tweet-preprocessor/#description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed63101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
